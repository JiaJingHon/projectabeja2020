{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plants-final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlo30sGx_6_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nykUvWND68O-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hERbgNCz7C_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af4pnfObhfuh",
        "colab_type": "text"
      },
      "source": [
        "#Download Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqgfqsxnhfKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash \n",
        "#run like a terminal\n",
        "pip uninstall -y kaggle\n",
        "pip install --upgrade pip\n",
        "pip install kaggle==1.5.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqpX-1XqqWKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets list -s plant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SsorK7UpClp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "0d4c0a7a-5f31-4239-bc77-b6ce6a4598a6"
      },
      "source": [
        "!kaggle datasets download -d vbookshelf/v2-plant-seedlings-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading v2-plant-seedlings-dataset.zip to /content\n",
            "100% 3.18G/3.19G [01:11<00:00, 59.9MB/s]\n",
            "100% 3.19G/3.19G [01:11<00:00, 48.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0txHVANGWT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q v2-plant-seedlings-dataset.zip -d \"/content/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5imvUsams8Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_path=os.path.join(\"/content\",\n",
        "                      \"data\",\n",
        "                      \"nonsegmentedv2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBUBqBWtnMG",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7EERCs2to80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure,axis=plt.subplots(nrows=3,ncols=4,figsize=(10,10))\n",
        "i=0\n",
        "j=0\n",
        "for filename in os.listdir(img_path):\n",
        "  curr_path=img_path+\"/\"+filename\n",
        "  label=filename\n",
        "\n",
        "  for img in os.listdir(curr_path):\n",
        "    image_path=curr_path+\"/\"+img\n",
        "    curr_img=np.asarray(Image.open(image_path))\n",
        "    print(curr_img.shape)\n",
        "    #plt.imshow(curr_img)\n",
        "    axis[i][j].imshow(curr_img)\n",
        "    axis[i][j].set_title(label)\n",
        "    j+=1\n",
        "    if j==4:\n",
        "      i+=1\n",
        "      j=0  \n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej7GRiuuzBEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_list,img_count_list=[],[]\n",
        "for filename in os.listdir(img_path):\n",
        "  curr_path=img_path+\"/\"+filename\n",
        "  label=filename\n",
        "  label_list.append(label)\n",
        "  img_count_list.append(len(os.listdir(curr_path)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwMrF_hyzEHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.bar(x=label_list,\n",
        "        height=img_count_list)\n",
        "plt.xticks(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhvKVk0g_Hgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "percentage_class_dict={}\n",
        "for i in range(12):\n",
        "  percentage_class_dict[label_list[i]]=round((img_count_list[i]/sum(img_count_list))*100,2)\n",
        "percentage_class_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jznslHhHr2dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To get the mean and standard deviation of dataset\n",
        "first_channel,second_channel,third_channel,num=[],[],[],0\n",
        "for filename in os.listdir(img_path):\n",
        "  curr_path=img_path+\"/\"+filename\n",
        "  for img in os.listdir(curr_path):\n",
        "    image_path=curr_path +\"/\" +img\n",
        "    img = np.array(PIL.Image.open(image_path).resize((244,244),Image.BILINEAR))\n",
        "    first_channel.append(img[:,:,0])\n",
        "    second_channel.append(img[:,:,1])\n",
        "    third_channel.append(img[:,:,2])\n",
        "    num+=1\n",
        "  print(filename,\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkXYoF6hr_N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean,std=[np.mean(first_channel)/255.,np.mean(second_channel)/255.,np.mean(third_channel)/255.],[np.std(first_channel)/255.,np.std(second_channel)/255.,np.std(third_channel)/255.]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_gBNRHIVngi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(mean,std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ3WSeY4ADUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean,std=[0.32878234546825347, 0.28885041498392117, 0.20677955249812788],[0.1033289967821012, 0.1086720358391526, 0.12568620125984942]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Y5SEgyViRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function returns a nested list of length 2, containing a list of image paths and its labels\n",
        "# training= 80%\n",
        "# validation= 10%\n",
        "# test set= 10%\n",
        "def prepare_data(path):\n",
        "  imgs_train,labels_train=[],[]\n",
        "  imgs_valid,labels_valid=[],[]\n",
        "  imgs_test,labels_test=[],[]\n",
        "  num_grayscale=0\n",
        "\n",
        "  for filename in os.listdir(path):\n",
        "    train,valid=True,True\n",
        "    label=filename\n",
        "    curr_path=path+\"/\"+label\n",
        "    \n",
        "\n",
        "    num_images= len(os.listdir(curr_path))\n",
        "    print(num_images)\n",
        "    num_train=math.floor(num_images*0.80)\n",
        "    num_valid=math.floor(num_images*0.10)\n",
        "    n=0\n",
        "\n",
        "    for img in os.listdir(curr_path):\n",
        "      my_img=np.asarray(Image.open(curr_path+\"/\"+img))\n",
        "\n",
        "      #Ensures that the images are all non grayscale images\n",
        "      if my_img.shape[2]==3:\n",
        "        if train:\n",
        "          imgs_train.append(curr_path+\"/\"+img)\n",
        "          labels_train.append(label)\n",
        "          n+=1\n",
        "\n",
        "          if n==num_train:\n",
        "            n=0\n",
        "            train=False\n",
        "\n",
        "        elif valid:\n",
        "          imgs_valid.append(curr_path+\"/\"+img)\n",
        "          labels_valid.append(label)\n",
        "          n+=1\n",
        "\n",
        "          if n==num_valid:\n",
        "            n=0\n",
        "            valid=False\n",
        "\n",
        "        else:\n",
        "          imgs_test.append(curr_path+\"/\"+img)\n",
        "          labels_test.append(label)\n",
        "\n",
        "      else:\n",
        "        num_grayscale+=1\n",
        "\n",
        "  return [imgs_train,labels_train],[imgs_valid,labels_valid],[imgs_test,labels_test],num_grayscale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCN8TxpoGR03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_prior,valid_prior,test_prior,num_gs_images=prepare_data(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul87QVBLKTf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(num_gs_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgrGc-DDX1nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_dictionary={'Maize':0,\n",
        "                  'Cleavers':1,\n",
        "                  'Sugar beet':2,\n",
        "                  'Common Chickweed':3,\n",
        "                  'Black-grass':4,\n",
        "                  'Scentless Mayweed':5,\n",
        "                  'Small-flowered Cranesbill':6,\n",
        "                  'ShepherdтАЩs Purse':7,\n",
        "                  'Loose Silky-bent':8,\n",
        "                  'Common wheat':9,\n",
        "                  'Charlock':10,\n",
        "                  'Fat Hen':11}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VAE1nqYJgXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GetDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,prior,mean,std,dict,set):\n",
        "    self.img_paths=prior[0]\n",
        "    self.labels=prior[1]\n",
        "    self.set=set\n",
        "    self.mean=mean\n",
        "    self.std=std\n",
        "    self.dict=dict\n",
        "    if self.set==\"train\":\n",
        "      self.transform=transforms.Compose([\n",
        "                                      transforms.Resize((512,512)),\n",
        "                                      transforms.RandomHorizontalFlip(0.5),\n",
        "                                      transforms.ColorJitter(brightness=0.4,contrast=0.4),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(self.mean,self.std)])\n",
        "      \n",
        "    else:\n",
        "      self.transform=transforms.Compose([transforms.Resize((512,512)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(self.mean,self.std)])   \n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img_path=self.img_paths[index]\n",
        "    label=self.labels[index]\n",
        "    label=self.dict.get(label)\n",
        "    img=Image.open(img_path)\n",
        "    img=self.transform(img)\n",
        "    \n",
        "    return [img,label]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKEcfdrmj9cW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset=GetDataset(train_prior,mean,std,class_dictionary,set=\"train\")\n",
        "valid_dataset=GetDataset(valid_prior,mean,std,class_dictionary,set=\"valid\")\n",
        "test_dataset=GetDataset(test_prior,mean,std,class_dictionary,set=\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhsPGy6-5wHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(valid_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpYl8x_Zeye5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=32,num_workers=4,shuffle=True)\n",
        "valid_loader=DataLoader(valid_dataset,batch_size=32,num_workers=4,shuffle=False)\n",
        "test_loader=DataLoader(test_dataset,batch_size=32,num_workers=4,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzt1vZW-Ykud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To visualise images after transformation in train set\n",
        "for i,data in enumerate(train_loader):\n",
        "  img,label=data\n",
        "  plt.imshow(np.transpose(np.array(img[10]),[1,2,0]))\n",
        "  print(label[3])\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGbnpPl8ZUoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To visualise images after transformation in validation set\n",
        "for i,data in enumerate(valid_loader):\n",
        "  img,label=data\n",
        "  plt.imshow(np.transpose(np.array(img[3]),[1,2,0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q4kJK6i03Cd",
        "colab_type": "text"
      },
      "source": [
        "#Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n34J-Z0iAqUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet50=torchvision.models.resnet50(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Blm9wVccRoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, child in resnet50.named_children():\n",
        "    for name2, params in child.named_parameters():\n",
        "        print(name, name2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeCduvAMOlsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "  def forward(self,x):\n",
        "    return x.reshape(x.size(0),-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESSJUAA7HmLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net_num_features=resnet50.fc.in_features\n",
        "\n",
        "myresnet50=nn.Sequential(resnet50.conv1,\n",
        "                         resnet50.bn1,\n",
        "                         resnet50.relu,\n",
        "                         resnet50.maxpool,\n",
        "                         nn.Sequential(*resnet50.layer1),\n",
        "                         nn.Sequential(*resnet50.layer2),\n",
        "                         nn.Sequential(*resnet50.layer3),\n",
        "                         nn.Sequential(*resnet50.layer4),\n",
        "                         resnet50.avgpool,\n",
        "                         Flatten(),\n",
        "                         nn.Dropout(0.5),\n",
        "                         nn.Linear(net_num_features,12,bias=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lY3EqQQFxdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, child in myresnet50.named_children():\n",
        "    for name2, params in child.named_parameters():\n",
        "        print(name, name2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRgMFZodXDro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myresnet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqmJMwHl51y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in myresnet50.parameters():\n",
        "  param.requires_grad=False\n",
        "\n",
        "#Unfreeze layers from the bottom\n",
        "myresnet50[11].weight.requires_grad=True\n",
        "myresnet50[11].bias.requires_grad=True\n",
        "\n",
        "myresnet50[7][2].conv3.weight.requires_grad=True\n",
        "myresnet50[7][2].bn3.weight.requires_grad=True\n",
        "myresnet50[7][2].bn3.bias.requires_grad=True\n",
        "\n",
        "myresnet50[7][2].conv2.weight.requires_grad=True\n",
        "myresnet50[7][2].bn2.weight.requires_grad=True\n",
        "myresnet50[7][2].bn2.bias.requires_grad=True\n",
        "\n",
        "myresnet50[7][2].conv1.weight.requires_grad=True\n",
        "myresnet50[7][2].bn1.weight.requires_grad=True\n",
        "myresnet50[7][2].bn1.bias.requires_grad=True\n",
        "\n",
        "myresnet50[7][1].conv3.weight.requires_grad=True\n",
        "myresnet50[7][1].bn3.weight.requires_grad=True\n",
        "myresnet50[7][1].bn3.bias.requires_grad=True\n",
        "\n",
        "myresnet50[7][1].conv2.weight.requires_grad=True\n",
        "myresnet50[7][1].bn2.weight.requires_grad=True\n",
        "myresnet50[7][1].bn2.bias.requires_grad=True\n",
        "\n",
        "myresnet50[7][1].conv1.weight.requires_grad=True\n",
        "myresnet50[7][1].bn1.weight.requires_grad=True\n",
        "myresnet50[7][1].bn1.bias.requires_grad=True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cnfbGoxGp39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myresnet50=myresnet50.to(torch.device(\"cuda:0\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60DNW_mlx1WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights=[]\n",
        "class_list=list(class_dictionary.keys())\n",
        "for i in range(12):\n",
        "  curr_class=class_list[i]\n",
        "  class_weights.append(1/percentage_class_dict[curr_class])\n",
        "\n",
        "class_weights=torch.tensor(class_weights)  \n",
        "class_weights=class_weights.to(torch.device(\"cuda:0\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2mucXQp8Twt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeKiNQPEDNqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func=nn.CrossEntropyLoss(weight=class_weights)\n",
        "device=torch.device(\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtJh7UVf6B8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train function with scheduled learning rate\n",
        "def train_scheduled(net,valid,loss_func,epochs,learning_rate,weight_decay,device,run):\n",
        "  print(\"Training the model...\")\n",
        "  net.to(device)\n",
        "  optimizer=optim.Adam(net.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "  scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=30)\n",
        "  comment=f'learning_rate={learning_rate},weight_decay ={weight_decay},run= {run}'\n",
        "  writer=SummaryWriter(comment=comment)\n",
        "  start=time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_loss,valid_loss,train_acc,valid_acc,n=0.0,0.0,0.0,0.0,0.0\n",
        "    for i,data in enumerate(train_loader):\n",
        "      img,label=data\n",
        "      img,label=img.to(device),label.to(device)\n",
        "      net.train()\n",
        "      optimizer.zero_grad()\n",
        "      label_preds=net(img)\n",
        "      loss=loss_func(label_preds,label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        label=label.long()\n",
        "        train_loss+=loss.float()\n",
        "        train_acc+=torch.sum(torch.argmax(label_preds,dim=1)==label).float()\n",
        "        n+=label.shape[0]\n",
        "\n",
        "    train_loss=train_loss/n\n",
        "    train_acc=train_acc/n\n",
        "    n=0\n",
        "\n",
        "    if valid:\n",
        "      net.eval()\n",
        "      for i,data in enumerate(valid_loader):\n",
        "        img,label=data\n",
        "        img,label=img.to(device),label.to(device)\n",
        "        label_preds=net(img)\n",
        "        loss=loss_func(label_preds,label)\n",
        "        valid_loss+=loss\n",
        "        valid_acc+=torch.sum(torch.argmax(label_preds,dim=1)==label).float()\n",
        "        n+=label.shape[0]\n",
        "\n",
        "    valid_loss=valid_loss/n\n",
        "    valid_acc=valid_acc/n\n",
        "\n",
        "    writer.add_scalars(main_tag=\"Loss\",tag_scalar_dict={\"Train loss\":train_loss,\n",
        "                              \"Valid loss\":valid_loss},global_step=epoch)\n",
        "    writer.add_scalars(main_tag=\"Accuracy\",tag_scalar_dict={\"Train accuracy\":train_acc,\n",
        "                              \"Valid accuracy\":valid_acc},global_step=epoch)\n",
        "    writer.add_scalar(\"Learning rate\",scheduler.get_last_lr()[0],epoch)\n",
        "  \n",
        "    #if epoch==0 or (epoch+1)%5==0:\n",
        "    if True:\n",
        "      print(\"Epoch %d: Learning rate is: %f, Train Accuracy is: %f, Train Loss is: %f, Valid Accuracy is %f, Valid Loss is %f\" %(epoch+1,scheduler.get_last_lr()[0],train_acc,train_loss,valid_acc,valid_loss))\n",
        "    scheduler.step()\n",
        "  end=time.time()\n",
        "  Runtime=end-start\n",
        "  print(\"Runtime is: %f\" % (Runtime))\n",
        "\n",
        "  writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cWkxhTkIh44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train function where learning rate decreases by a factor of 10 every 5 epochs\n",
        "def train(net,valid,loss_func,epochs,learning_rate,weight_decay,device,run):\n",
        "  print(\"Training the model...\")\n",
        "  net.to(device)\n",
        "  optimizer=optim.Adam(net.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "  comment=f'learning_rate={learning_rate},weight_decay ={weight_decay},run= {run}'\n",
        "  writer=SummaryWriter(comment=comment)\n",
        "  start=time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_loss,valid_loss,train_acc,valid_acc,n=0.0,0.0,0.0,0.0,0.0\n",
        "    if (epoch+1)%5==0:\n",
        "      learning_rate=learning_rate/10\n",
        "      optimizer=optim.Adam(net.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "    for i,data in enumerate(train_loader):\n",
        "      img,label=data\n",
        "      img,label=img.to(device),label.to(device)\n",
        "      net.train()\n",
        "      optimizer.zero_grad()\n",
        "      label_preds=net(img)\n",
        "      loss=loss_func(label_preds,label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        label=label.long()\n",
        "        train_loss+=loss.float()\n",
        "        train_acc+=torch.sum(torch.argmax(label_preds,dim=1)==label).float()\n",
        "        n+=label.shape[0]\n",
        "\n",
        "    train_loss=train_loss/n\n",
        "    train_acc=train_acc/n\n",
        "    n=0\n",
        "\n",
        "    if valid:\n",
        "      net.eval()\n",
        "      with torch.no_grad():\n",
        "        for i,data in enumerate(valid_loader):\n",
        "          img,label=data\n",
        "          img,label=img.to(device),label.to(device)\n",
        "          label_preds=net(img)\n",
        "          loss=loss_func(label_preds,label)\n",
        "          valid_loss+=loss\n",
        "          valid_acc+=torch.sum(torch.argmax(label_preds,dim=1)==label).float()\n",
        "          n+=label.shape[0]\n",
        "\n",
        "    valid_loss=valid_loss/n\n",
        "    valid_acc=valid_acc/n\n",
        "\n",
        "    writer.add_scalars(main_tag=\"Loss\",tag_scalar_dict={\"Train loss\":train_loss,\n",
        "                              \"Valid loss\":valid_loss},global_step=epoch)\n",
        "    writer.add_scalars(main_tag=\"Accuracy\",tag_scalar_dict={\"Train accuracy\":train_acc,\n",
        "                              \"Valid accuracy\":valid_acc},global_step=epoch)\n",
        "    writer.add_scalar(\"Learning rate\",learning_rate,epoch)\n",
        "    end=time.time()\n",
        "    Runtime=end-start\n",
        "\n",
        "    if True:\n",
        "      print(\"Epoch %d:, learning rate is: %f, Train Accuracy is: %f, Train Loss is: %f, Valid Accuracy is %f, Valid Loss is %f\" %(epoch+1,learning_rate,train_acc,train_loss,valid_acc,valid_loss))\n",
        "      print(\"Run time is :%f\"%(Runtime))\n",
        "\n",
        "  writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlojNmBdIsyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialising weights for final fully connected layer.\n",
        "nn.init.xavier_uniform_(myresnet50[11].weight);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNyGLwEdzPZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(myresnet50,True,loss_func,30,0.01,0.00001,device,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CjMmSJJ0_1D",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZTlcYXrGRzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to get test accuracy, and list of wrongly classified images and labels             \n",
        "def test_accuracy():\n",
        "  myresnet50.eval()\n",
        "  test_acc,n=0.0,0\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(test_loader):\n",
        "      img,label=data\n",
        "      img,label=img.to(torch.device(\"cuda:0\")),label.to(torch.device(\"cuda:0\"))\n",
        "      label_pred=myresnet50(img)\n",
        "      test_acc+=torch.sum(torch.argmax(label_pred,dim=1)==label).float()\n",
        "      n+=label.shape[0]\n",
        "      \n",
        "      #To get wrongly classified images:\n",
        "      labels=torch.argmax(label_pred,dim=1)\n",
        "      for i in range(label.shape[0]):\n",
        "        if labels[i]!=label[i]:\n",
        "          wrongly_classified.append([img[i],labels[i],label[i]])\n",
        "          class_count[int(labels[i])]+=1\n",
        "\n",
        "  test_acc=test_acc/n\n",
        "  print(\"Test Accuracy is: %f\"% (test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31Q3s9CoIfuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrongly_classified=[]\n",
        "class_count={0:0,\n",
        "             1:0,\n",
        "             2:0,\n",
        "             3:0,\n",
        "             4:0,\n",
        "             5:0,\n",
        "             6:0,\n",
        "             7:0,\n",
        "             8:0,\n",
        "             9:0,\n",
        "             10:0,\n",
        "             11:0}\n",
        "             \n",
        "test_accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhB8U_XdIQnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEsnJvoiJzfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_to_labels={0:'Maize',\n",
        "                 1:'Cleavers',\n",
        "                 2:'Sugar beet',\n",
        "                 3:'Common Chickweed',\n",
        "                 4: 'Black-grass',\n",
        "                 5:'Scentless Mayweed',\n",
        "                 6: 'Small-flowered Cranesbill',\n",
        "                 7:'ShepherdтАЩs Purse',\n",
        "                 8:'Loose Silky-bent',\n",
        "                 9:'Common wheat',\n",
        "                 10:'Charlock',\n",
        "                 11:'Fat Hen'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QRoP0G5j_HB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To unnormalize image for plotting\n",
        "unnormalize= transforms.Normalize(\n",
        "   mean=[-0.32878234546825347/0.1033289967821012, -0.28885041498392117/0.1086720358391526, -0.20677955249812788/0.12568620125984942],\n",
        "   std=[1/0.1033289967821012, 1/0.1086720358391526, 1/0.12568620125984942]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlTHkDclISg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot out images that are wrongly classified as well as their predicted and actual label\n",
        "fig,axis=plt.subplots(nrows=4,ncols=5,figsize=(22,15))\n",
        "r=0\n",
        "c=0\n",
        "for i in range(len(wrongly_classified)):\n",
        "  img=np.asarray(unnormalize(wrongly_classified[i][0]).to(torch.device(\"cpu\")))\n",
        "  index=wrongly_classified[i][1]\n",
        "  actual_index=wrongly_classified[i][2]\n",
        "  label=index_to_labels.get(int(index))\n",
        "  actual_label=index_to_labels.get(int(actual_index))\n",
        "  axis[r][c].imshow(np.transpose(img,[1,2,0]))\n",
        "  axis[r][c].imshow(np.transpose(img,[1,2,0]))\n",
        "  axis[r][c].set_title(\"Predicted: \"+label+\"\\n\"+\"Actual \"+actual_label)\n",
        "  axis[r][c].set_xticks([])\n",
        "  axis[r][c].set_yticks([])\n",
        "  c+=1\n",
        "  if c==5:\n",
        "      r+=1\n",
        "      c=0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka4aeYJvcqqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUmo6gLBneMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(myresnet50.state_dict(),\"/content/drive/My Drive/plant_seedling/paths/37run.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtyJk01AVXW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyHhiCHBVrNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to get 2 tensors of actual and predicted classes of all images in the test set\n",
        "def actual_predicted():\n",
        "  myresnet50.eval()\n",
        "  actual=None\n",
        "  predicted=None\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(test_loader):\n",
        "      img,label=data\n",
        "      img,label=img.to(torch.device(\"cuda:0\")),label.to(torch.device(\"cuda:0\"))\n",
        "      label_pred=myresnet50(img)\n",
        "      label_pred=torch.argmax(label_pred,dim=1)\n",
        "\n",
        "      if actual==None:\n",
        "        actual=label\n",
        "      else:\n",
        "        actual=torch.cat((actual,label))\n",
        "      \n",
        "      if predicted==None:\n",
        "        predicted= label_pred\n",
        "      else:\n",
        "        predicted=torch.cat((predicted,label_pred))\n",
        "\n",
        "  return actual.to(torch.device(\"cpu\")),predicted.to(torch.device(\"cpu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw55hIFOWYRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual,predicted=actual_predicted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFt7lANIVYfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm=confusion_matrix(actual,predicted) # np array \n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66iwSE8rY-yJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision_score(actual,\n",
        "                predicted,\n",
        "                average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK_w6GR1oGZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion Matrix for predicted and actual classes for images in the test set\n",
        "figure,axis=plt.subplots(figsize=(10,10))\n",
        "mat=axis.imshow(cm)\n",
        "\n",
        "axis.set_xticks(np.arange(12))\n",
        "axis.set_yticks(np.arange(12))\n",
        "\n",
        "axis.set_xticklabels(list(class_dictionary.keys()),\n",
        "                     rotation=90)\n",
        "axis.set_yticklabels(list(class_dictionary.keys()))\n",
        "\n",
        "for i in range(12):\n",
        "  for j in range(12):\n",
        "    axis.text(i,\n",
        "              j,\n",
        "              cm[i,j],\n",
        "              ha=\"center\",\n",
        "              va=\"center\",\n",
        "              color=\"crimson\",\n",
        "              fontsize=15)\n",
        "\n",
        "axis.set_title(\"Confusion matrix\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.xlabel(\"Actual\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}